{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX4keHU4XIYr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "-> Information Gain is a metric used in decision trees to measure how well a feature splits the data into meaningful classes. It is based on the reduction in entropy (uncertainty) after splitting the dataset.\n",
        "\n",
        "Entropy measures impurity or disorder in data.\n",
        "\n",
        "    Information Gain = Entropy(before split) ‚àí Weighted Entropy(after split)\n",
        "\n",
        "A feature with higher Information Gain is more useful for classification and is selected as a split node in the decision tree. Decision tree algorithms like ID3 and C4.5 use Information Gain to choose the best attribute at each step.\n",
        "\n",
        "#2. Difference between Gini Impurity and Entropy\n",
        "\n",
        "\n",
        "    Feature     \t     Gini Impurity              \tEntropy\n",
        "\n",
        "    Definition\tMeasures probability of incorrect classification\tMeasures amount of uncertainty\n",
        "    \n",
        "    Formula\t             1‚àí‚àëùëùùëñ2                       ‚àí‚àëpi log2(pi)\n",
        "\n",
        "\t‚Äã Range            \t  0 to 0.5\t                      0 to 1\n",
        "   \n",
        "    Speed\t              Faster to compute\t       More computationally expensive\n",
        "   \n",
        "    Used In\t            CART (Classification and Regression Trees)\t  ID3 / C4.5 Decision Trees\n",
        "    \n",
        "    Preferred When\t    Want speed and simplicity\tWhen probabilistic purity\n",
        "    matters\n",
        "\n",
        "\n",
        "#Q3. What is Pre-Pruning in Decision Trees?\n",
        "->Pre-pruning (early stopping) stops the tree from growing too deep during training by applying constraints like:\n",
        "\n",
        "Maximum depth\n",
        "\n",
        "Minimum samples per leaf or node\n",
        "\n",
        "Minimum information gain\n",
        "\n",
        "Maximum number of leaves\n",
        "\n",
        "It prevents overfitting by stopping unnecessary splits and improves model generalization and training speed.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7flg2qDiXLL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Decision Tree model with Gini impurity\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(data.feature_names, model.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgro8ao-ZgDW",
        "outputId": "29fc1915-9f95-4f36-ed88-f4799f071e23"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "->SVM is a supervised ML algorithm used for classification and regression. It finds a maximum-margin hyperplane that best separates classes in feature space.\n",
        "\n",
        "Key ideas:\n",
        "\n",
        "Maximizes margin between classes\n",
        "\n",
        "Uses support vectors (critical points near boundary)\n",
        "\n",
        "Works well in high-dimensional data\n",
        "\n",
        "#Q6: What is the Kernel Trick in SVM?\n",
        "\n",
        "-> The Kernel Trick allows SVM to handle non-linear datasets by transforming data into a higher-dimensional space without explicitly computing the transformation.\n",
        "\n",
        "Popular kernels:\n",
        "\n",
        "-> Linear\n",
        "\n",
        "-> Polynomial\n",
        "\n",
        "-> RBF (Radial Basis Function)\n",
        "\n",
        "-> Sigmoid\n",
        "\n",
        "It enables SVM to classify data that is not linearly separable."
      ],
      "metadata": {
        "id": "XI0XN9qHZ2xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear SVM\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "pred_linear = svm_linear.predict(X_test)\n",
        "\n",
        "# RBF SVM\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Linear Kernel Accuracy:\", accuracy_score(y_test, pred_linear))\n",
        "print(\"RBF Kernel Accuracy:\", accuracy_score(y_test, pred_rbf))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOUvcUwmaO2B",
        "outputId": "a2d1e1d2-1d1c-4c98-a1d7-dc63c62ae322"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 1.0\n",
            "RBF Kernel Accuracy: 0.8055555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q8: What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n",
        "\n",
        "->Na√Øve Bayes is a probabilistic classifier based on Bayes' Theorem.\n",
        "It assumes features are independent given the class label ‚Äî this assumption is often unrealistic, hence the term \"Na√Øve\".\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Fast and efficient\n",
        "\n",
        "Works well with text (spam filtering, sentiment analysis)\n",
        "\n",
        "Performs well on high-dimensional data\n",
        "\n",
        "\n",
        "\n",
        "#Q9: Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√ØveBayes, and Bernoulli Na√Øve Bayes\n",
        "\n",
        "| Model              | Data Type             | Use Case                             |\n",
        "| ------------------ | --------------------- | ------------------------------------ |\n",
        "| **Gaussian NB**    | Continuous features   | Iris dataset, medical data           |\n",
        "| **Multinomial NB** | Count-based features  | Text classification, word counts     |\n",
        "| **Bernoulli NB**   | Binary features (0/1) | Binary text features, spam filtering |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Tjw2O8l6avXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q10.Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Gaussian Naive Bayes model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Prediction & accuracy\n",
        "pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8x2BeIqZmnw",
        "outputId": "b42cf47c-655b-4128-b4e9-ffe947032b47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RRmTp9kgbb50"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}